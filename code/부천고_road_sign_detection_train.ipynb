{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StKBkrptVQWW"
      },
      "source": [
        "## Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Wlnzfq0pLBH8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from IPython.display import Image\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dropout, Dense\n",
        "\n",
        "#ImageDataGenerator\n",
        "training_datagen = ImageDataGenerator(\n",
        "    rescale=1. / 255,        #rescale: 이미지의 픽셀 값을 조정\n",
        "    width_shift_range=0.2,   #width_shift_range: 가로 방향으로 이동\n",
        "    height_shift_range=0.2,  #height_shift_range: 세로 방향으로 이동\n",
        "    zoom_range = 0.2,        #zoom_range: 이미지 확대\n",
        "    fill_mode = 'reflect',   #fill_mode: 이미지를 이동이나 굴절시켰을 때 빈 픽셀 값에 대하여 값을 채우는 방식\n",
        "    validation_split=0.1,    #validation_split: validation set의 구성 비율\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wc2sry-YVhmq"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CBfOAkenTV0j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59ff61bd-4587-4bd2-f954-9c2e229421de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory not Found and make director\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "\n",
        "def DeleteAllFiles(filePath):\n",
        "    if os.path.exists(filePath):\n",
        "        for file in os.scandir(filePath):\n",
        "            os.remove(file.path)\n",
        "    else:\n",
        "        os.makedirs(filePath)\n",
        "        return 'Directory not Found and make director'        \n",
        "\n",
        "TRAINING_DIR = \"/content/train_data/\"  \n",
        "print(DeleteAllFiles(TRAINING_DIR))  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KnkPO026TYI2"
      },
      "outputs": [],
      "source": [
        "#압축풀기\n",
        "local_zip = 'train_data.zip' \n",
        "image_file_path = '/content/train_data/'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall(image_file_path)\n",
        "zip_ref.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GMQmAaPATZZ0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6702529a-0e72-4515-ad56-50923fea2eef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2351 images belonging to 118 classes.\n"
          ]
        }
      ],
      "source": [
        "training_generator = training_datagen.flow_from_directory(TRAINING_DIR, \n",
        "                                                          batch_size=4, \n",
        "                                                          target_size=(150, 150), \n",
        "                                                          class_mode='categorical', \n",
        "                                                          subset='training',\n",
        "                                                         )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KC21gB7zTad0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "007ef079-ccd5-4139-b9ec-d4e85907950f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 208 images belonging to 118 classes.\n"
          ]
        }
      ],
      "source": [
        "validation_generator = training_datagen.flow_from_directory(TRAINING_DIR, \n",
        "                                                            batch_size=4, \n",
        "                                                            target_size=(150, 150), \n",
        "                                                            class_mode='categorical',\n",
        "                                                            subset='validation', \n",
        "                                                          )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "dir_path = '/content/train_data'\n",
        "file_list = []\n",
        "\n",
        "for (root, directories, files) in os.walk(dir_path):\n",
        "    for file in files:\n",
        "        file_path = os.path.join(root, file)\n",
        "        file_list.append(file_path[20:23])\n",
        "\n",
        "print(file_list)\n",
        "\n",
        "my_set = set(file_list)\n",
        "test_files = list(my_set)\n",
        "\n",
        "files = [int (i) for i in test_files]\n",
        "\n",
        "print(len(files))"
      ],
      "metadata": {
        "id": "ZzZPV6WyaFTN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f383943f-a7f9-45b2-b748-60fe4ee36104"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['210', '219', '219', '219', '219', '219', '219', '219', '219', '219', '219', '219', '219', '219', '219', '219', '219', '219', '219', '219', '219', '219', '219', '219', '219', '219', '219', '219', '219', '202', '202', '202', '202', '202', '202', '318', '102', '102', '102', '102', '135', '135', '135', '135', '135', '135', '135', '135', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '129', '129', '129', '129', '129', '129', '129', '129', '129', '129', '129', '129', '129', '129', '129', '129', '129', '129', '129', '129', '129', '129', '129', '129', '129', '129', '129', '129', '129', '129', '129', '129', '129', '129', '129', '129', '129', '129', '129', '129', '129', '129', '129', '129', '109', '109', '109', '101', '101', '101', '101', '101', '101', '101', '101', '101', '101', '101', '304', '304', '304', '304', '304', '304', '304', '427', '427', '427', '427', '227', '227', '227', '227', '227', '227', '227', '227', '227', '227', '227', '227', '227', '227', '227', '227', '227', '227', '227', '227', '227', '227', '227', '227', '227', '227', '227', '325', '325', '325', '325', '325', '325', '325', '325', '325', '325', '325', '325', '325', '325', '325', '325', '325', '325', '118', '118', '428', '428', '428', '428', '428', '428', '428', '428', '428', '428', '428', '428', '428', '428', '428', '428', '428', '428', '428', '428', '428', '428', '428', '428', '428', '428', '428', '428', '428', '428', '428', '428', '428', '428', '428', '428', '428', '428', '428', '428', '428', '428', '428', '428', '428', '428', '428', '428', '428', '428', '428', '428', '428', '428', '428', '428', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '329', '329', '329', '329', '329', '329', '329', '329', '329', '329', '329', '329', '329', '329', '329', '329', '329', '329', '329', '329', '329', '329', '329', '329', '329', '329', '329', '329', '329', '329', '329', '329', '329', '329', '329', '329', '329', '329', '329', '329', '329', '329', '329', '329', '329', '329', '329', '329', '329', '329', '329', '329', '329', '328', '328', '328', '328', '328', '328', '328', '328', '328', '106', '106', '106', '106', '224', '224', '206', '117', '117', '224', '224', '224', '224', '123', '123', '123', '123', '123', '123', '123', '123', '123', '123', '213', '213', '213', '213', '213', '213', '213', '213', '213', '213', '213', '213', '213', '213', '213', '213', '213', '213', '213', '213', '213', '213', '213', '213', '213', '213', '213', '213', '213', '213', '213', '213', '213', '213', '213', '213', '213', '213', '213', '213', '213', '213', '213', '213', '213', '213', '213', '213', '213', '213', '213', '213', '213', '213', '213', '213', '213', '213', '213', '213', '213', '213', '213', '213', '213', '213', '213', '213', '213', '213', '213', '213', '213', '213', '213', '213', '213', '306', '306', '306', '306', '306', '306', '306', '306', '306', '306', '306', '306', '306', '306', '306', '306', '306', '306', '306', '306', '306', '306', '120', '120', '222', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '311', '127', '127', '127', '127', '127', '127', '127', '127', '127', '113', '113', '113', '113', '313', '313', '313', '313', '313', '313', '313', '313', '313', '313', '313', '313', '313', '313', '313', '313', '313', '313', '313', '313', '313', '313', '313', '313', '313', '313', '313', '313', '313', '313', '313', '313', '313', '313', '313', '313', '313', '313', '313', '313', '313', '417', '417', '308', '308', '308', '308', '308', '308', '308', '308', '308', '308', '308', '308', '308', '308', '308', '308', '308', '308', '308', '308', '308', '308', '308', '308', '308', '308', '308', '308', '308', '308', '308', '308', '308', '308', '308', '308', '308', '308', '308', '308', '308', '308', '308', '308', '308', '308', '308', '308', '308', '308', '308', '308', '308', '308', '308', '308', '308', '308', '308', '308', '308', '308', '308', '308', '308', '308', '308', '308', '308', '308', '308', '308', '308', '308', '308', '231', '125', '125', '125', '125', '125', '125', '125', '125', '125', '125', '125', '125', '125', '125', '125', '125', '125', '125', '125', '125', '125', '125', '125', '125', '125', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '214', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '139', '139', '139', '139', '139', '305', '305', '305', '305', '305', '305', '305', '305', '305', '305', '305', '305', '305', '320', '309', '309', '309', '309', '309', '309', '309', '309', '309', '309', '309', '309', '309', '309', '309', '309', '309', '309', '309', '309', '309', '309', '309', '309', '309', '309', '309', '309', '309', '309', '309', '309', '309', '309', '309', '309', '309', '309', '309', '309', '309', '309', '309', '211', '211', '211', '211', '211', '211', '211', '211', '211', '211', '211', '211', '211', '211', '418', '418', '418', '418', '418', '112', '112', '112', '112', '112', '112', '112', '112', '112', '112', '112', '112', '112', '112', '112', '112', '112', '112', '112', '112', '112', '112', '112', '112', '225', '225', '225', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '218', '221', '221', '221', '221', '221', '221', '221', '221', '221', '221', '221', '221', '221', '221', '221', '221', '221', '221', '221', '221', '221', '221', '221', '221', '221', '221', '221', '221', '221', '221', '221', '221', '221', '221', '221', '221', '221', '221', '221', '221', '221', '221', '221', '221', '221', '221', '221', '221', '221', '221', '221', '221', '221', '221', '221', '221', '221', '221', '221', '321', '316', '316', '316', '316', '316', '316', '316', '316', '316', '316', '316', '316', '316', '316', '316', '316', '316', '115', '115', '225', '225', '225', '225', '225', '225', '225', '225', '225', '323', '323', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '122', '122', '122', '122', '105', '105', '105', '105', '105', '105', '105', '128', '128', '104', '104', '104', '104', '104', '130', '130', '130', '130', '130', '228', '228', '228', '228', '228', '228', '228', '228', '228', '228', '228', '228', '228', '228', '228', '228', '228', '228', '228', '103', '103', '103', '226', '226', '226', '226', '226', '226', '226', '226', '226', '226', '226', '226', '226', '226', '226', '226', '226', '226', '226', '226', '226', '226', '226', '226', '226', '226', '226', '226', '226', '226', '226', '226', '226', '226', '226', '226', '226', '226', '226', '226', '226', '226', '226', '226', '226', '226', '226', '226', '226', '226', '226', '307', '307', '307', '307', '307', '307', '307', '307', '307', '307', '307', '307', '307', '307', '307', '307', '307', '307', '307', '307', '307', '307', '307', '307', '411', '411', '411', '411', '411', '411', '411', '411', '119', '119', '119', '119', '119', '119', '119', '119', '119', '119', '119', '119', '119', '119', '119', '119', '119', '212', '212', '212', '212', '212', '212', '212', '212', '212', '212', '212', '212', '212', '212', '212', '212', '212', '212', '212', '212', '212', '212', '212', '212', '212', '212', '212', '212', '212', '212', '212', '212', '212', '212', '212', '319', '319', '220', '220', '220', '220', '220', '220', '220', '220', '412', '412', '330', '330', '330', '330', '330', '330', '330', '330', '330', '330', '330', '330', '330', '330', '330', '330', '330', '330', '330', '330', '330', '330', '330', '330', '330', '330', '330', '330', '330', '330', '330', '207', '134', '134', '134', '134', '134', '134', '134', '134', '134', '134', '134', '134', '134', '134', '425', '425', '425', '217', '217', '217', '217', '217', '217', '217', '217', '217', '217', '217', '217', '217', '217', '217', '217', '217', '217', '217', '217', '217', '217', '217', '217', '217', '217', '217', '217', '217', '217', '217', '415', '415', '415', '415', '415', '415', '415', '415', '415', '415', '415', '205', '205', '205', '205', '205', '205', '205', '205', '205', '205', '205', '205', '205', '205', '205', '205', '205', '205', '205', '205', '205', '205', '205', '205', '205', '205', '205', '205', '205', '205', '205', '205', '111', '111', '111', '111', '111', '111', '111', '111', '111', '111', '111', '111', '111', '111', '111', '111', '111', '111', '111', '111', '111', '111', '111', '126', '126', '126', '126', '126', '126', '126', '126', '126', '126', '126', '126', '126', '126', '126', '126', '126', '126', '126', '126', '126', '126', '126', '126', '126', '126', '126', '126', '126', '126', '126', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '322', '140', '140', '140', '140', '140', '140', '140', '140', '140', '140', '140', '140', '140', '140', '140', '140', '140', '140', '140', '140', '140', '140', '140', '140', '140', '140', '140', '140', '140', '140', '140', '140', '107', '107', '107', '107', '107', '107', '107', '107', '107', '107', '107', '107', '107', '107', '107', '107', '107', '107', '107', '107', '107', '107', '107', '107', '107', '107', '107', '107', '107', '107', '107', '107', '107', '107', '107', '107', '107', '223', '223', '223', '223', '223', '223', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '136', '136', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '331', '110', '110', '110', '110', '110', '225', '225', '225', '225', '310', '310', '310', '310', '216', '216', '216', '216', '216', '216', '216', '216', '216', '216', '216', '216', '216', '216', '216', '216', '216', '216', '216', '216', '216', '216', '216', '216', '216', '216', '216', '216', '216', '216', '216', '216', '216', '216', '216', '216', '216', '216', '216', '216', '216', '216', '216', '216', '216', '216', '216', '203', '203', '203', '203', '203', '203', '203', '203', '203', '108', '108', '108', '108', '108', '108', '108', '108', '108', '108', '419', '419', '419', '315', '315', '315', '315', '315', '315', '315', '315', '315', '315', '315', '133', '133', '133', '133', '133', '133', '133', '133', '133', '302', '302', '302', '302', '302', '302', '312', '312', '312', '312', '312', '312', '312', '312', '312', '312', '312', '312', '312', '312', '312', '312', '312', '312', '312', '312', '312', '312', '312', '312', '312', '312', '312', '114', '114', '114', '114', '114', '114', '301', '301', '303', '303', '303', '303', '303', '303', '303', '303', '303', '303', '303', '303', '303', '303', '303', '303', '303', '303', '303', '303', '303', '303', '303', '303', '303', '314', '314', '314', '314', '314', '314', '314', '314', '314', '201', '201', '124', '124', '332', '204', '326', '326', '326', '326', '326', '326', '326', '326', '326', '326', '317', '317', '317', '317', '317', '317', '317', '317', '317', '317', '317', '317', '317', '317', '317', '116', '116', '116', '116', '116', '327', '327', '327', '327', '327', '327', '327', '327', '327', '327', '327', '327', '327', '327', '327', '327', '327', '138', '138', '138', '138', '138', '138', '138', '138', '138', '138', '138', '138', '137', '137', '137', '137', '137', '137', '137', '137', '137', '137', '137', '137', '137', '137', '324', '324', '324', '324', '324', '324', '324', '324', '324', '324', '324', '324', '324', '324', '324', '324', '324', '324', '324', '324', '324', '324', '324', '324', '324', '324', '324', '324', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '224', '121', '121', '132', '132', '132', '132', '132', '132', '132', '132', '132', '132', '132', '132', '132', '132', '132', '132', '132', '132', '132', '132', '132', '132', '132', '132', '132', '132', '132', '132', '132', '132', '132', '132', '132', '132', '132', '132', '132', '132', '132', '132', '132', '132', '132', '132', '132', '132', '132', '132', '132', '132', '132', '132', '132', '132', '132', '224', '224', '224', '230', '230', '230', '230', '230', '230', '230', '230', '230']\n",
            "107\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hs0lO6JIUnQk"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "J65nXrmbUPkg"
      },
      "outputs": [],
      "source": [
        "model = Sequential([\n",
        "    Conv2D(64, (3, 3), activation='PReLU', input_shape=(150, 150, 3)),\n",
        "    MaxPooling2D(2, 2), \n",
        "    Conv2D(128, (3, 3), activation='PReLU'),\n",
        "    MaxPooling2D(2, 2), \n",
        "    Conv2D(128, (3, 3), activation='PReLU'),\n",
        "    MaxPooling2D(2, 2), \n",
        "    Conv2D(128, (3, 3), activation='PReLU'),\n",
        "    MaxPooling2D(2, 2), \n",
        "    Conv2D(128, (3, 3), activation='softmax'),\n",
        "    MaxPooling2D(2, 2), \n",
        "\n",
        "\n",
        "    Flatten(), \n",
        "    Dropout(0.5),\n",
        "    Dense(512, activation='PReLU'),\n",
        "    Dense(128, activation='PReLU'),\n",
        "    Dense(32, activation='PReLU'),\n",
        "    Dense(118, activation='softmax'),   \n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0cxVJocEUl_S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ac73580-551f-4c15-ed17-f5ad74b44c70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 148, 148, 64)      1403648   \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 74, 74, 64)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 72, 72, 128)       737408    \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 36, 36, 128)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 34, 34, 128)       295552    \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 17, 17, 128)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 15, 15, 128)       176384    \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 7, 7, 128)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 5, 5, 128)         147584    \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPooling  (None, 2, 2, 128)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 512)               0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 512)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               263168    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 128)               65792     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 32)                4160      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 118)               3894      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,097,590\n",
            "Trainable params: 3,097,590\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x52bGw75VM0i"
      },
      "source": [
        "## Compile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "iRmwYKp8UvBA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f08461d2-68c1-4cd5-c9ed-fd77c8a60ee8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "model.compile(optimizer=Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['acc'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "UUBIj-AFUvU4"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = \"tmp_checkpoint.ckpt\"\n",
        "checkpoint = ModelCheckpoint(filepath=checkpoint_path, \n",
        "                             save_weights_only=True, \n",
        "                             save_best_only=True, \n",
        "                             monitor='val_loss', \n",
        "                             verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRNpaAq4bd6H"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "RO9EfSCLbdiK"
      },
      "outputs": [],
      "source": [
        "#학습 회수 선정\n",
        "epochs = 300"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "_wHfiKUvbprU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e26d186a-254b-4c1f-8ec5-8bf68b7e313a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 4.5161 - acc: 0.0460\n",
            "Epoch 1: val_loss improved from inf to 4.06225, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 26s 26ms/step - loss: 4.5164 - acc: 0.0459 - val_loss: 4.0622 - val_acc: 0.0913\n",
            "Epoch 2/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 4.2305 - acc: 0.0724\n",
            "Epoch 2: val_loss improved from 4.06225 to 3.99930, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 4.2299 - acc: 0.0732 - val_loss: 3.9993 - val_acc: 0.0913\n",
            "Epoch 3/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 4.1487 - acc: 0.0733\n",
            "Epoch 3: val_loss improved from 3.99930 to 3.92007, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 4.1490 - acc: 0.0732 - val_loss: 3.9201 - val_acc: 0.0913\n",
            "Epoch 4/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 4.1173 - acc: 0.0732\n",
            "Epoch 4: val_loss improved from 3.92007 to 3.88856, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 4.1173 - acc: 0.0732 - val_loss: 3.8886 - val_acc: 0.0913\n",
            "Epoch 5/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 4.0498 - acc: 0.0750\n",
            "Epoch 5: val_loss improved from 3.88856 to 3.78871, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 4.0493 - acc: 0.0749 - val_loss: 3.7887 - val_acc: 0.0913\n",
            "Epoch 6/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 3.9985 - acc: 0.0770\n",
            "Epoch 6: val_loss improved from 3.78871 to 3.70340, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 3.9985 - acc: 0.0770 - val_loss: 3.7034 - val_acc: 0.1058\n",
            "Epoch 7/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 3.9235 - acc: 0.0927\n",
            "Epoch 7: val_loss improved from 3.70340 to 3.65020, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 3.9235 - acc: 0.0927 - val_loss: 3.6502 - val_acc: 0.1250\n",
            "Epoch 8/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 3.8336 - acc: 0.1097\n",
            "Epoch 8: val_loss improved from 3.65020 to 3.52974, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 3.8336 - acc: 0.1097 - val_loss: 3.5297 - val_acc: 0.1394\n",
            "Epoch 9/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 3.7206 - acc: 0.1297\n",
            "Epoch 9: val_loss improved from 3.52974 to 3.40086, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 3.7206 - acc: 0.1297 - val_loss: 3.4009 - val_acc: 0.1731\n",
            "Epoch 10/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 3.6005 - acc: 0.1444\n",
            "Epoch 10: val_loss improved from 3.40086 to 3.17377, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 3.5993 - acc: 0.1442 - val_loss: 3.1738 - val_acc: 0.1827\n",
            "Epoch 11/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 3.5060 - acc: 0.1440\n",
            "Epoch 11: val_loss improved from 3.17377 to 3.11830, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 3.5041 - acc: 0.1438 - val_loss: 3.1183 - val_acc: 0.1875\n",
            "Epoch 12/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 3.4550 - acc: 0.1460\n",
            "Epoch 12: val_loss improved from 3.11830 to 3.07101, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 3.4520 - acc: 0.1463 - val_loss: 3.0710 - val_acc: 0.1923\n",
            "Epoch 13/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 3.3953 - acc: 0.1474\n",
            "Epoch 13: val_loss improved from 3.07101 to 3.00780, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 3.3938 - acc: 0.1480 - val_loss: 3.0078 - val_acc: 0.1923\n",
            "Epoch 14/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 3.3420 - acc: 0.1483\n",
            "Epoch 14: val_loss did not improve from 3.00780\n",
            "588/588 [==============================] - 14s 25ms/step - loss: 3.3425 - acc: 0.1480 - val_loss: 3.0310 - val_acc: 0.1971\n",
            "Epoch 15/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 3.2832 - acc: 0.1581\n",
            "Epoch 15: val_loss improved from 3.00780 to 2.97390, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 3.2851 - acc: 0.1578 - val_loss: 2.9739 - val_acc: 0.2163\n",
            "Epoch 16/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 3.2491 - acc: 0.1672\n",
            "Epoch 16: val_loss improved from 2.97390 to 2.91310, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 3.2491 - acc: 0.1672 - val_loss: 2.9131 - val_acc: 0.2260\n",
            "Epoch 17/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 3.2399 - acc: 0.1700\n",
            "Epoch 17: val_loss improved from 2.91310 to 2.84565, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 3.2403 - acc: 0.1697 - val_loss: 2.8457 - val_acc: 0.2356\n",
            "Epoch 18/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 3.1915 - acc: 0.1754\n",
            "Epoch 18: val_loss did not improve from 2.84565\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 3.1921 - acc: 0.1752 - val_loss: 2.9613 - val_acc: 0.2308\n",
            "Epoch 19/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 3.1836 - acc: 0.1734\n",
            "Epoch 19: val_loss improved from 2.84565 to 2.79740, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 3.1824 - acc: 0.1740 - val_loss: 2.7974 - val_acc: 0.2404\n",
            "Epoch 20/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 3.1175 - acc: 0.1883\n",
            "Epoch 20: val_loss did not improve from 2.79740\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 3.1186 - acc: 0.1880 - val_loss: 2.8283 - val_acc: 0.2404\n",
            "Epoch 21/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 3.0968 - acc: 0.1930\n",
            "Epoch 21: val_loss improved from 2.79740 to 2.73333, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 3.0956 - acc: 0.1931 - val_loss: 2.7333 - val_acc: 0.2500\n",
            "Epoch 22/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 3.0648 - acc: 0.1964\n",
            "Epoch 22: val_loss improved from 2.73333 to 2.71317, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 3.0644 - acc: 0.1965 - val_loss: 2.7132 - val_acc: 0.2452\n",
            "Epoch 23/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 3.0396 - acc: 0.2020\n",
            "Epoch 23: val_loss did not improve from 2.71317\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 3.0396 - acc: 0.2020 - val_loss: 2.7778 - val_acc: 0.2212\n",
            "Epoch 24/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 3.0019 - acc: 0.2020\n",
            "Epoch 24: val_loss improved from 2.71317 to 2.65142, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 3.0021 - acc: 0.2016 - val_loss: 2.6514 - val_acc: 0.2596\n",
            "Epoch 25/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 2.9720 - acc: 0.2096\n",
            "Epoch 25: val_loss improved from 2.65142 to 2.63045, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 2.9716 - acc: 0.2101 - val_loss: 2.6304 - val_acc: 0.2452\n",
            "Epoch 26/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 2.9523 - acc: 0.2105\n",
            "Epoch 26: val_loss improved from 2.63045 to 2.60797, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 2.9513 - acc: 0.2110 - val_loss: 2.6080 - val_acc: 0.3077\n",
            "Epoch 27/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 2.9181 - acc: 0.2164\n",
            "Epoch 27: val_loss did not improve from 2.60797\n",
            "588/588 [==============================] - 14s 25ms/step - loss: 2.9192 - acc: 0.2165 - val_loss: 2.6082 - val_acc: 0.2981\n",
            "Epoch 28/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 2.9071 - acc: 0.2284\n",
            "Epoch 28: val_loss improved from 2.60797 to 2.50280, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 2.9057 - acc: 0.2288 - val_loss: 2.5028 - val_acc: 0.3173\n",
            "Epoch 29/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 2.8515 - acc: 0.2293\n",
            "Epoch 29: val_loss improved from 2.50280 to 2.47067, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 2.8515 - acc: 0.2293 - val_loss: 2.4707 - val_acc: 0.3221\n",
            "Epoch 30/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 2.8237 - acc: 0.2326\n",
            "Epoch 30: val_loss did not improve from 2.47067\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 2.8272 - acc: 0.2322 - val_loss: 2.5054 - val_acc: 0.2788\n",
            "Epoch 31/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 2.8076 - acc: 0.2433\n",
            "Epoch 31: val_loss improved from 2.47067 to 2.42340, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 2.8079 - acc: 0.2433 - val_loss: 2.4234 - val_acc: 0.3462\n",
            "Epoch 32/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 2.7764 - acc: 0.2569\n",
            "Epoch 32: val_loss improved from 2.42340 to 2.35859, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 2.7754 - acc: 0.2565 - val_loss: 2.3586 - val_acc: 0.3125\n",
            "Epoch 33/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 2.7502 - acc: 0.2505\n",
            "Epoch 33: val_loss did not improve from 2.35859\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 2.7466 - acc: 0.2514 - val_loss: 2.3858 - val_acc: 0.3462\n",
            "Epoch 34/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 2.7125 - acc: 0.2625\n",
            "Epoch 34: val_loss improved from 2.35859 to 2.29207, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 2.7127 - acc: 0.2620 - val_loss: 2.2921 - val_acc: 0.3221\n",
            "Epoch 35/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 2.6869 - acc: 0.2689\n",
            "Epoch 35: val_loss did not improve from 2.29207\n",
            "588/588 [==============================] - 14s 25ms/step - loss: 2.6879 - acc: 0.2684 - val_loss: 2.4449 - val_acc: 0.3269\n",
            "Epoch 36/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 2.7219 - acc: 0.2719\n",
            "Epoch 36: val_loss improved from 2.29207 to 2.24903, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 2.7226 - acc: 0.2718 - val_loss: 2.2490 - val_acc: 0.3365\n",
            "Epoch 37/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 2.6588 - acc: 0.2833\n",
            "Epoch 37: val_loss did not improve from 2.24903\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 2.6588 - acc: 0.2833 - val_loss: 2.2996 - val_acc: 0.3462\n",
            "Epoch 38/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 2.6147 - acc: 0.2799\n",
            "Epoch 38: val_loss did not improve from 2.24903\n",
            "588/588 [==============================] - 14s 25ms/step - loss: 2.6164 - acc: 0.2799 - val_loss: 2.3042 - val_acc: 0.3510\n",
            "Epoch 39/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 2.6020 - acc: 0.2975\n",
            "Epoch 39: val_loss improved from 2.24903 to 2.24403, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 2.6014 - acc: 0.2969 - val_loss: 2.2440 - val_acc: 0.3510\n",
            "Epoch 40/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 2.5752 - acc: 0.2876\n",
            "Epoch 40: val_loss improved from 2.24403 to 2.16501, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 2.5750 - acc: 0.2880 - val_loss: 2.1650 - val_acc: 0.3413\n",
            "Epoch 41/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 2.5893 - acc: 0.2996\n",
            "Epoch 41: val_loss did not improve from 2.16501\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 2.5914 - acc: 0.2994 - val_loss: 2.2242 - val_acc: 0.3798\n",
            "Epoch 42/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 2.5536 - acc: 0.3029\n",
            "Epoch 42: val_loss improved from 2.16501 to 2.05883, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 2.5543 - acc: 0.3028 - val_loss: 2.0588 - val_acc: 0.3558\n",
            "Epoch 43/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 2.5211 - acc: 0.3077\n",
            "Epoch 43: val_loss did not improve from 2.05883\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 2.5241 - acc: 0.3071 - val_loss: 2.0702 - val_acc: 0.3894\n",
            "Epoch 44/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 2.4753 - acc: 0.3244\n",
            "Epoch 44: val_loss did not improve from 2.05883\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 2.4762 - acc: 0.3241 - val_loss: 2.0984 - val_acc: 0.3798\n",
            "Epoch 45/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 2.4926 - acc: 0.3098\n",
            "Epoch 45: val_loss improved from 2.05883 to 2.01982, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 2.4911 - acc: 0.3101 - val_loss: 2.0198 - val_acc: 0.3846\n",
            "Epoch 46/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 2.4320 - acc: 0.3360\n",
            "Epoch 46: val_loss did not improve from 2.01982\n",
            "588/588 [==============================] - 14s 25ms/step - loss: 2.4320 - acc: 0.3360 - val_loss: 2.0855 - val_acc: 0.3990\n",
            "Epoch 47/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 2.4185 - acc: 0.3308\n",
            "Epoch 47: val_loss improved from 2.01982 to 1.91288, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 2.4182 - acc: 0.3309 - val_loss: 1.9129 - val_acc: 0.4279\n",
            "Epoch 48/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 2.4099 - acc: 0.3210\n",
            "Epoch 48: val_loss did not improve from 1.91288\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 2.4097 - acc: 0.3211 - val_loss: 2.0472 - val_acc: 0.3846\n",
            "Epoch 49/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 2.3628 - acc: 0.3359\n",
            "Epoch 49: val_loss did not improve from 1.91288\n",
            "588/588 [==============================] - 14s 25ms/step - loss: 2.3630 - acc: 0.3356 - val_loss: 2.0211 - val_acc: 0.3894\n",
            "Epoch 50/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 2.3699 - acc: 0.3427\n",
            "Epoch 50: val_loss did not improve from 1.91288\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 2.3675 - acc: 0.3437 - val_loss: 1.9998 - val_acc: 0.4038\n",
            "Epoch 51/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 2.3406 - acc: 0.3483\n",
            "Epoch 51: val_loss improved from 1.91288 to 1.90526, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 2.3410 - acc: 0.3479 - val_loss: 1.9053 - val_acc: 0.3942\n",
            "Epoch 52/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 2.2778 - acc: 0.3530\n",
            "Epoch 52: val_loss did not improve from 1.90526\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 2.2763 - acc: 0.3539 - val_loss: 1.9676 - val_acc: 0.3942\n",
            "Epoch 53/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 2.2721 - acc: 0.3683\n",
            "Epoch 53: val_loss did not improve from 1.90526\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 2.2711 - acc: 0.3684 - val_loss: 1.9402 - val_acc: 0.4471\n",
            "Epoch 54/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 2.2786 - acc: 0.3579\n",
            "Epoch 54: val_loss improved from 1.90526 to 1.88505, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 2.2778 - acc: 0.3586 - val_loss: 1.8851 - val_acc: 0.4663\n",
            "Epoch 55/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 2.2315 - acc: 0.3653\n",
            "Epoch 55: val_loss improved from 1.88505 to 1.81882, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 2.2325 - acc: 0.3650 - val_loss: 1.8188 - val_acc: 0.4519\n",
            "Epoch 56/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 2.2086 - acc: 0.3794\n",
            "Epoch 56: val_loss did not improve from 1.81882\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 2.2086 - acc: 0.3794 - val_loss: 1.9074 - val_acc: 0.4135\n",
            "Epoch 57/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 2.2298 - acc: 0.3758\n",
            "Epoch 57: val_loss did not improve from 1.81882\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 2.2293 - acc: 0.3760 - val_loss: 1.9273 - val_acc: 0.4231\n",
            "Epoch 58/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 2.1546 - acc: 0.3805\n",
            "Epoch 58: val_loss improved from 1.81882 to 1.70820, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 2.1523 - acc: 0.3807 - val_loss: 1.7082 - val_acc: 0.4663\n",
            "Epoch 59/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 2.1789 - acc: 0.3747\n",
            "Epoch 59: val_loss did not improve from 1.70820\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 2.1803 - acc: 0.3743 - val_loss: 1.7656 - val_acc: 0.4375\n",
            "Epoch 60/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 2.1391 - acc: 0.3956\n",
            "Epoch 60: val_loss did not improve from 1.70820\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 2.1387 - acc: 0.3964 - val_loss: 1.7227 - val_acc: 0.4663\n",
            "Epoch 61/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 2.1408 - acc: 0.3934\n",
            "Epoch 61: val_loss did not improve from 1.70820\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 2.1408 - acc: 0.3934 - val_loss: 1.7222 - val_acc: 0.4952\n",
            "Epoch 62/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 2.0949 - acc: 0.3982\n",
            "Epoch 62: val_loss did not improve from 1.70820\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 2.0946 - acc: 0.3986 - val_loss: 1.7433 - val_acc: 0.4952\n",
            "Epoch 63/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 2.1022 - acc: 0.3950\n",
            "Epoch 63: val_loss improved from 1.70820 to 1.69628, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 2.1006 - acc: 0.3956 - val_loss: 1.6963 - val_acc: 0.5000\n",
            "Epoch 64/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 2.1104 - acc: 0.3933\n",
            "Epoch 64: val_loss did not improve from 1.69628\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 2.1115 - acc: 0.3930 - val_loss: 1.7404 - val_acc: 0.5144\n",
            "Epoch 65/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 2.0743 - acc: 0.3973\n",
            "Epoch 65: val_loss improved from 1.69628 to 1.65108, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 2.0743 - acc: 0.3973 - val_loss: 1.6511 - val_acc: 0.4808\n",
            "Epoch 66/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 2.0519 - acc: 0.4156\n",
            "Epoch 66: val_loss did not improve from 1.65108\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 2.0519 - acc: 0.4156 - val_loss: 1.7248 - val_acc: 0.4519\n",
            "Epoch 67/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 2.0284 - acc: 0.4042\n",
            "Epoch 67: val_loss improved from 1.65108 to 1.64422, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 2.0271 - acc: 0.4049 - val_loss: 1.6442 - val_acc: 0.5192\n",
            "Epoch 68/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 2.0294 - acc: 0.4286\n",
            "Epoch 68: val_loss did not improve from 1.64422\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 2.0294 - acc: 0.4288 - val_loss: 1.6825 - val_acc: 0.5000\n",
            "Epoch 69/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 1.9814 - acc: 0.4312\n",
            "Epoch 69: val_loss did not improve from 1.64422\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.9810 - acc: 0.4313 - val_loss: 1.7492 - val_acc: 0.4952\n",
            "Epoch 70/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 1.9793 - acc: 0.4262\n",
            "Epoch 70: val_loss did not improve from 1.64422\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.9793 - acc: 0.4262 - val_loss: 1.6610 - val_acc: 0.5240\n",
            "Epoch 71/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 1.9685 - acc: 0.4269\n",
            "Epoch 71: val_loss improved from 1.64422 to 1.57422, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.9675 - acc: 0.4275 - val_loss: 1.5742 - val_acc: 0.5048\n",
            "Epoch 72/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 1.9586 - acc: 0.4410\n",
            "Epoch 72: val_loss did not improve from 1.57422\n",
            "588/588 [==============================] - 14s 25ms/step - loss: 1.9581 - acc: 0.4411 - val_loss: 1.6391 - val_acc: 0.5192\n",
            "Epoch 73/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 1.9417 - acc: 0.4330\n",
            "Epoch 73: val_loss improved from 1.57422 to 1.53911, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.9417 - acc: 0.4330 - val_loss: 1.5391 - val_acc: 0.5481\n",
            "Epoch 74/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 1.9029 - acc: 0.4478\n",
            "Epoch 74: val_loss did not improve from 1.53911\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.9021 - acc: 0.4475 - val_loss: 1.5704 - val_acc: 0.5144\n",
            "Epoch 75/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 1.8894 - acc: 0.4597\n",
            "Epoch 75: val_loss improved from 1.53911 to 1.50011, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.8878 - acc: 0.4598 - val_loss: 1.5001 - val_acc: 0.5288\n",
            "Epoch 76/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 1.8662 - acc: 0.4530\n",
            "Epoch 76: val_loss did not improve from 1.50011\n",
            "588/588 [==============================] - 14s 25ms/step - loss: 1.8662 - acc: 0.4530 - val_loss: 1.6980 - val_acc: 0.5288\n",
            "Epoch 77/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 1.8660 - acc: 0.4487\n",
            "Epoch 77: val_loss improved from 1.50011 to 1.49314, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.8674 - acc: 0.4487 - val_loss: 1.4931 - val_acc: 0.5385\n",
            "Epoch 78/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 1.8835 - acc: 0.4589\n",
            "Epoch 78: val_loss did not improve from 1.49314\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.8824 - acc: 0.4594 - val_loss: 1.5127 - val_acc: 0.5481\n",
            "Epoch 79/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 1.8242 - acc: 0.4631\n",
            "Epoch 79: val_loss improved from 1.49314 to 1.48285, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.8229 - acc: 0.4632 - val_loss: 1.4829 - val_acc: 0.5048\n",
            "Epoch 80/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 1.8103 - acc: 0.4661\n",
            "Epoch 80: val_loss improved from 1.48285 to 1.42280, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.8118 - acc: 0.4653 - val_loss: 1.4228 - val_acc: 0.5481\n",
            "Epoch 81/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 1.7977 - acc: 0.4653\n",
            "Epoch 81: val_loss did not improve from 1.42280\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.7977 - acc: 0.4653 - val_loss: 1.4847 - val_acc: 0.5481\n",
            "Epoch 82/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 1.7942 - acc: 0.4806\n",
            "Epoch 82: val_loss did not improve from 1.42280\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.7922 - acc: 0.4811 - val_loss: 1.4608 - val_acc: 0.5625\n",
            "Epoch 83/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 1.7652 - acc: 0.4891\n",
            "Epoch 83: val_loss did not improve from 1.42280\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.7645 - acc: 0.4896 - val_loss: 1.6050 - val_acc: 0.5337\n",
            "Epoch 84/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 1.7771 - acc: 0.4848\n",
            "Epoch 84: val_loss improved from 1.42280 to 1.41849, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.7790 - acc: 0.4849 - val_loss: 1.4185 - val_acc: 0.5337\n",
            "Epoch 85/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 1.7447 - acc: 0.4806\n",
            "Epoch 85: val_loss did not improve from 1.41849\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.7442 - acc: 0.4806 - val_loss: 1.4587 - val_acc: 0.5769\n",
            "Epoch 86/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 1.7625 - acc: 0.4879\n",
            "Epoch 86: val_loss did not improve from 1.41849\n",
            "588/588 [==============================] - 14s 25ms/step - loss: 1.7625 - acc: 0.4879 - val_loss: 1.4624 - val_acc: 0.5769\n",
            "Epoch 87/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 1.7110 - acc: 0.4934\n",
            "Epoch 87: val_loss did not improve from 1.41849\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.7133 - acc: 0.4926 - val_loss: 1.4856 - val_acc: 0.5673\n",
            "Epoch 88/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 1.7323 - acc: 0.4904\n",
            "Epoch 88: val_loss improved from 1.41849 to 1.39903, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.7346 - acc: 0.4896 - val_loss: 1.3990 - val_acc: 0.5865\n",
            "Epoch 89/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 1.6819 - acc: 0.5006\n",
            "Epoch 89: val_loss did not improve from 1.39903\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.6819 - acc: 0.5006 - val_loss: 1.5235 - val_acc: 0.5385\n",
            "Epoch 90/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 1.6432 - acc: 0.5109\n",
            "Epoch 90: val_loss did not improve from 1.39903\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.6409 - acc: 0.5117 - val_loss: 1.5467 - val_acc: 0.5385\n",
            "Epoch 91/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 1.7103 - acc: 0.4925\n",
            "Epoch 91: val_loss did not improve from 1.39903\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.7091 - acc: 0.4930 - val_loss: 1.5376 - val_acc: 0.5337\n",
            "Epoch 92/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 1.6707 - acc: 0.5190\n",
            "Epoch 92: val_loss improved from 1.39903 to 1.33358, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.6714 - acc: 0.5185 - val_loss: 1.3336 - val_acc: 0.5673\n",
            "Epoch 93/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 1.6791 - acc: 0.5185\n",
            "Epoch 93: val_loss did not improve from 1.33358\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.6791 - acc: 0.5185 - val_loss: 1.3926 - val_acc: 0.5913\n",
            "Epoch 94/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 1.6108 - acc: 0.5284\n",
            "Epoch 94: val_loss did not improve from 1.33358\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.6090 - acc: 0.5287 - val_loss: 1.4039 - val_acc: 0.6154\n",
            "Epoch 95/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 1.6212 - acc: 0.5151\n",
            "Epoch 95: val_loss did not improve from 1.33358\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.6212 - acc: 0.5151 - val_loss: 1.3635 - val_acc: 0.6058\n",
            "Epoch 96/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 1.6289 - acc: 0.5284\n",
            "Epoch 96: val_loss did not improve from 1.33358\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.6286 - acc: 0.5287 - val_loss: 1.3391 - val_acc: 0.6010\n",
            "Epoch 97/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 1.6155 - acc: 0.5233\n",
            "Epoch 97: val_loss did not improve from 1.33358\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.6162 - acc: 0.5232 - val_loss: 1.3796 - val_acc: 0.6106\n",
            "Epoch 98/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 1.5571 - acc: 0.5381\n",
            "Epoch 98: val_loss did not improve from 1.33358\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.5571 - acc: 0.5381 - val_loss: 1.3877 - val_acc: 0.6010\n",
            "Epoch 99/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 1.5709 - acc: 0.5479\n",
            "Epoch 99: val_loss improved from 1.33358 to 1.29280, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.5709 - acc: 0.5479 - val_loss: 1.2928 - val_acc: 0.6010\n",
            "Epoch 100/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 1.5413 - acc: 0.5483\n",
            "Epoch 100: val_loss did not improve from 1.29280\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.5413 - acc: 0.5483 - val_loss: 1.2984 - val_acc: 0.6250\n",
            "Epoch 101/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 1.5403 - acc: 0.5505\n",
            "Epoch 101: val_loss did not improve from 1.29280\n",
            "588/588 [==============================] - 14s 25ms/step - loss: 1.5423 - acc: 0.5500 - val_loss: 1.3389 - val_acc: 0.6250\n",
            "Epoch 102/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 1.5354 - acc: 0.5407\n",
            "Epoch 102: val_loss did not improve from 1.29280\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.5384 - acc: 0.5398 - val_loss: 1.3665 - val_acc: 0.6154\n",
            "Epoch 103/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 1.5026 - acc: 0.5500\n",
            "Epoch 103: val_loss did not improve from 1.29280\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.5026 - acc: 0.5500 - val_loss: 1.3495 - val_acc: 0.6298\n",
            "Epoch 104/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 1.5299 - acc: 0.5526\n",
            "Epoch 104: val_loss improved from 1.29280 to 1.26278, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.5303 - acc: 0.5530 - val_loss: 1.2628 - val_acc: 0.6442\n",
            "Epoch 105/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 1.4918 - acc: 0.5599\n",
            "Epoch 105: val_loss did not improve from 1.26278\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.4938 - acc: 0.5593 - val_loss: 1.2690 - val_acc: 0.6106\n",
            "Epoch 106/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 1.4514 - acc: 0.5705\n",
            "Epoch 106: val_loss improved from 1.26278 to 1.18738, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.4514 - acc: 0.5708 - val_loss: 1.1874 - val_acc: 0.6442\n",
            "Epoch 107/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 1.5031 - acc: 0.5470\n",
            "Epoch 107: val_loss did not improve from 1.18738\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.5031 - acc: 0.5470 - val_loss: 1.2370 - val_acc: 0.6779\n",
            "Epoch 108/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 1.4467 - acc: 0.5674\n",
            "Epoch 108: val_loss improved from 1.18738 to 1.18003, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.4467 - acc: 0.5674 - val_loss: 1.1800 - val_acc: 0.6538\n",
            "Epoch 109/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 1.4422 - acc: 0.5785\n",
            "Epoch 109: val_loss improved from 1.18003 to 1.13560, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.4422 - acc: 0.5785 - val_loss: 1.1356 - val_acc: 0.6779\n",
            "Epoch 110/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 1.4326 - acc: 0.5756\n",
            "Epoch 110: val_loss did not improve from 1.13560\n",
            "588/588 [==============================] - 14s 25ms/step - loss: 1.4337 - acc: 0.5751 - val_loss: 1.1767 - val_acc: 0.6202\n",
            "Epoch 111/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 1.4048 - acc: 0.5874\n",
            "Epoch 111: val_loss did not improve from 1.13560\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.4048 - acc: 0.5874 - val_loss: 1.1943 - val_acc: 0.6394\n",
            "Epoch 112/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 1.3937 - acc: 0.5756\n",
            "Epoch 112: val_loss improved from 1.13560 to 1.12901, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.3928 - acc: 0.5759 - val_loss: 1.1290 - val_acc: 0.6683\n",
            "Epoch 113/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 1.4200 - acc: 0.5803\n",
            "Epoch 113: val_loss did not improve from 1.12901\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.4234 - acc: 0.5793 - val_loss: 1.2299 - val_acc: 0.6298\n",
            "Epoch 114/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 1.3683 - acc: 0.5904\n",
            "Epoch 114: val_loss did not improve from 1.12901\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.3683 - acc: 0.5904 - val_loss: 1.2334 - val_acc: 0.6538\n",
            "Epoch 115/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 1.3553 - acc: 0.5980\n",
            "Epoch 115: val_loss did not improve from 1.12901\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.3576 - acc: 0.5972 - val_loss: 1.1584 - val_acc: 0.6635\n",
            "Epoch 116/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 1.3569 - acc: 0.6002\n",
            "Epoch 116: val_loss did not improve from 1.12901\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.3569 - acc: 0.6002 - val_loss: 1.1655 - val_acc: 0.6010\n",
            "Epoch 117/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 1.3640 - acc: 0.5897\n",
            "Epoch 117: val_loss improved from 1.12901 to 1.08162, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.3625 - acc: 0.5900 - val_loss: 1.0816 - val_acc: 0.6779\n",
            "Epoch 118/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 1.3208 - acc: 0.6031\n",
            "Epoch 118: val_loss did not improve from 1.08162\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.3216 - acc: 0.6023 - val_loss: 1.0839 - val_acc: 0.7019\n",
            "Epoch 119/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 1.3192 - acc: 0.6046\n",
            "Epoch 119: val_loss improved from 1.08162 to 1.04297, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.3198 - acc: 0.6044 - val_loss: 1.0430 - val_acc: 0.7115\n",
            "Epoch 120/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 1.2920 - acc: 0.6182\n",
            "Epoch 120: val_loss did not improve from 1.04297\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.2905 - acc: 0.6185 - val_loss: 1.1793 - val_acc: 0.6587\n",
            "Epoch 121/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 1.3361 - acc: 0.6019\n",
            "Epoch 121: val_loss did not improve from 1.04297\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.3361 - acc: 0.6019 - val_loss: 1.0912 - val_acc: 0.6587\n",
            "Epoch 122/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 1.3018 - acc: 0.6125\n",
            "Epoch 122: val_loss did not improve from 1.04297\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.3018 - acc: 0.6125 - val_loss: 1.1235 - val_acc: 0.6683\n",
            "Epoch 123/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 1.2989 - acc: 0.6144\n",
            "Epoch 123: val_loss improved from 1.04297 to 0.99948, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.2983 - acc: 0.6142 - val_loss: 0.9995 - val_acc: 0.7067\n",
            "Epoch 124/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 1.2385 - acc: 0.6253\n",
            "Epoch 124: val_loss improved from 0.99948 to 0.95114, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.2385 - acc: 0.6253 - val_loss: 0.9511 - val_acc: 0.7115\n",
            "Epoch 125/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 1.2928 - acc: 0.6084\n",
            "Epoch 125: val_loss did not improve from 0.95114\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.2918 - acc: 0.6091 - val_loss: 1.1302 - val_acc: 0.7067\n",
            "Epoch 126/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 1.2188 - acc: 0.6270\n",
            "Epoch 126: val_loss did not improve from 0.95114\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.2183 - acc: 0.6265 - val_loss: 1.0870 - val_acc: 0.6635\n",
            "Epoch 127/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 1.2237 - acc: 0.6321\n",
            "Epoch 127: val_loss did not improve from 0.95114\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.2238 - acc: 0.6321 - val_loss: 1.1203 - val_acc: 0.6779\n",
            "Epoch 128/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 1.2767 - acc: 0.6129\n",
            "Epoch 128: val_loss did not improve from 0.95114\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.2767 - acc: 0.6129 - val_loss: 1.0470 - val_acc: 0.7356\n",
            "Epoch 129/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 1.2377 - acc: 0.6402\n",
            "Epoch 129: val_loss did not improve from 0.95114\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.2377 - acc: 0.6402 - val_loss: 1.1267 - val_acc: 0.6971\n",
            "Epoch 130/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 1.2319 - acc: 0.6270\n",
            "Epoch 130: val_loss did not improve from 0.95114\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.2316 - acc: 0.6265 - val_loss: 1.1123 - val_acc: 0.7067\n",
            "Epoch 131/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 1.2225 - acc: 0.6321\n",
            "Epoch 131: val_loss did not improve from 0.95114\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.2225 - acc: 0.6321 - val_loss: 1.0948 - val_acc: 0.6827\n",
            "Epoch 132/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 1.1999 - acc: 0.6338\n",
            "Epoch 132: val_loss did not improve from 0.95114\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.1999 - acc: 0.6338 - val_loss: 1.0132 - val_acc: 0.7163\n",
            "Epoch 133/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 1.1505 - acc: 0.6589\n",
            "Epoch 133: val_loss did not improve from 0.95114\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.1505 - acc: 0.6589 - val_loss: 1.1836 - val_acc: 0.6490\n",
            "Epoch 134/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 1.1624 - acc: 0.6620\n",
            "Epoch 134: val_loss did not improve from 0.95114\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.1612 - acc: 0.6627 - val_loss: 1.0026 - val_acc: 0.7212\n",
            "Epoch 135/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 1.2088 - acc: 0.6340\n",
            "Epoch 135: val_loss did not improve from 0.95114\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.2093 - acc: 0.6338 - val_loss: 1.0655 - val_acc: 0.7404\n",
            "Epoch 136/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 1.1591 - acc: 0.6499\n",
            "Epoch 136: val_loss did not improve from 0.95114\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.1591 - acc: 0.6499 - val_loss: 1.0159 - val_acc: 0.7115\n",
            "Epoch 137/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 1.1313 - acc: 0.6543\n",
            "Epoch 137: val_loss improved from 0.95114 to 0.90290, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.1312 - acc: 0.6546 - val_loss: 0.9029 - val_acc: 0.7644\n",
            "Epoch 138/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 1.1617 - acc: 0.6402\n",
            "Epoch 138: val_loss did not improve from 0.90290\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.1617 - acc: 0.6402 - val_loss: 1.0730 - val_acc: 0.6923\n",
            "Epoch 139/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 1.1337 - acc: 0.6567\n",
            "Epoch 139: val_loss improved from 0.90290 to 0.85600, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.1337 - acc: 0.6567 - val_loss: 0.8560 - val_acc: 0.7596\n",
            "Epoch 140/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 1.1301 - acc: 0.6526\n",
            "Epoch 140: val_loss did not improve from 0.85600\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.1302 - acc: 0.6529 - val_loss: 0.9240 - val_acc: 0.7212\n",
            "Epoch 141/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 1.0942 - acc: 0.6681\n",
            "Epoch 141: val_loss did not improve from 0.85600\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.0955 - acc: 0.6678 - val_loss: 0.9447 - val_acc: 0.7356\n",
            "Epoch 142/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 1.1382 - acc: 0.6491\n",
            "Epoch 142: val_loss did not improve from 0.85600\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.1382 - acc: 0.6491 - val_loss: 0.9582 - val_acc: 0.7356\n",
            "Epoch 143/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 1.1081 - acc: 0.6830\n",
            "Epoch 143: val_loss did not improve from 0.85600\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.1082 - acc: 0.6827 - val_loss: 1.0464 - val_acc: 0.7163\n",
            "Epoch 144/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 1.0459 - acc: 0.6725\n",
            "Epoch 144: val_loss did not improve from 0.85600\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.0459 - acc: 0.6725 - val_loss: 0.9673 - val_acc: 0.7163\n",
            "Epoch 145/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 1.1157 - acc: 0.6614\n",
            "Epoch 145: val_loss improved from 0.85600 to 0.84169, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.1157 - acc: 0.6614 - val_loss: 0.8417 - val_acc: 0.7500\n",
            "Epoch 146/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 1.1427 - acc: 0.6530\n",
            "Epoch 146: val_loss did not improve from 0.84169\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.1423 - acc: 0.6538 - val_loss: 0.8566 - val_acc: 0.7548\n",
            "Epoch 147/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 1.0597 - acc: 0.6684\n",
            "Epoch 147: val_loss improved from 0.84169 to 0.81193, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.0590 - acc: 0.6682 - val_loss: 0.8119 - val_acc: 0.7548\n",
            "Epoch 148/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 1.0681 - acc: 0.6732\n",
            "Epoch 148: val_loss did not improve from 0.81193\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.0663 - acc: 0.6738 - val_loss: 0.9898 - val_acc: 0.7260\n",
            "Epoch 149/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 1.0928 - acc: 0.6685\n",
            "Epoch 149: val_loss did not improve from 0.81193\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.0913 - acc: 0.6691 - val_loss: 0.8403 - val_acc: 0.7548\n",
            "Epoch 150/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 1.0679 - acc: 0.6643\n",
            "Epoch 150: val_loss did not improve from 0.81193\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.0681 - acc: 0.6640 - val_loss: 0.9813 - val_acc: 0.7067\n",
            "Epoch 151/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 1.0346 - acc: 0.6844\n",
            "Epoch 151: val_loss did not improve from 0.81193\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.0346 - acc: 0.6844 - val_loss: 0.9303 - val_acc: 0.7452\n",
            "Epoch 152/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 1.0367 - acc: 0.6884\n",
            "Epoch 152: val_loss did not improve from 0.81193\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.0376 - acc: 0.6882 - val_loss: 0.9445 - val_acc: 0.7260\n",
            "Epoch 153/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 1.0087 - acc: 0.6937\n",
            "Epoch 153: val_loss did not improve from 0.81193\n",
            "588/588 [==============================] - 14s 25ms/step - loss: 1.0098 - acc: 0.6933 - val_loss: 0.9477 - val_acc: 0.7356\n",
            "Epoch 154/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 1.0222 - acc: 0.6867\n",
            "Epoch 154: val_loss did not improve from 0.81193\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.0213 - acc: 0.6874 - val_loss: 0.9509 - val_acc: 0.7019\n",
            "Epoch 155/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 1.0003 - acc: 0.6984\n",
            "Epoch 155: val_loss did not improve from 0.81193\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.0003 - acc: 0.6984 - val_loss: 1.1535 - val_acc: 0.7019\n",
            "Epoch 156/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 1.0286 - acc: 0.6876\n",
            "Epoch 156: val_loss did not improve from 0.81193\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.0285 - acc: 0.6878 - val_loss: 0.8492 - val_acc: 0.7788\n",
            "Epoch 157/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 0.9693 - acc: 0.7095\n",
            "Epoch 157: val_loss did not improve from 0.81193\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.9693 - acc: 0.7095 - val_loss: 0.9190 - val_acc: 0.7500\n",
            "Epoch 158/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 1.0243 - acc: 0.6831\n",
            "Epoch 158: val_loss did not improve from 0.81193\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.0243 - acc: 0.6831 - val_loss: 0.8684 - val_acc: 0.7933\n",
            "Epoch 159/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 0.9849 - acc: 0.6988\n",
            "Epoch 159: val_loss did not improve from 0.81193\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.9837 - acc: 0.6993 - val_loss: 0.8942 - val_acc: 0.7933\n",
            "Epoch 160/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 1.0044 - acc: 0.6941\n",
            "Epoch 160: val_loss did not improve from 0.81193\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.0046 - acc: 0.6937 - val_loss: 0.8505 - val_acc: 0.7740\n",
            "Epoch 161/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 1.0329 - acc: 0.6954\n",
            "Epoch 161: val_loss did not improve from 0.81193\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 1.0340 - acc: 0.6950 - val_loss: 0.9721 - val_acc: 0.7260\n",
            "Epoch 162/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 0.9778 - acc: 0.6979\n",
            "Epoch 162: val_loss did not improve from 0.81193\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.9779 - acc: 0.6976 - val_loss: 0.8268 - val_acc: 0.7692\n",
            "Epoch 163/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 0.9587 - acc: 0.7008\n",
            "Epoch 163: val_loss did not improve from 0.81193\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.9584 - acc: 0.7014 - val_loss: 0.9833 - val_acc: 0.7404\n",
            "Epoch 164/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 0.9873 - acc: 0.7090\n",
            "Epoch 164: val_loss did not improve from 0.81193\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.9900 - acc: 0.7078 - val_loss: 1.1364 - val_acc: 0.7115\n",
            "Epoch 165/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 0.9296 - acc: 0.7158\n",
            "Epoch 165: val_loss did not improve from 0.81193\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.9304 - acc: 0.7159 - val_loss: 0.8656 - val_acc: 0.7740\n",
            "Epoch 166/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 0.9401 - acc: 0.7021\n",
            "Epoch 166: val_loss did not improve from 0.81193\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.9403 - acc: 0.7018 - val_loss: 0.8707 - val_acc: 0.7788\n",
            "Epoch 167/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 0.9577 - acc: 0.7061\n",
            "Epoch 167: val_loss improved from 0.81193 to 0.75261, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.9577 - acc: 0.7061 - val_loss: 0.7526 - val_acc: 0.8029\n",
            "Epoch 168/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 0.9079 - acc: 0.7200\n",
            "Epoch 168: val_loss did not improve from 0.75261\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.9104 - acc: 0.7197 - val_loss: 0.9153 - val_acc: 0.7740\n",
            "Epoch 169/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 0.9326 - acc: 0.7133\n",
            "Epoch 169: val_loss did not improve from 0.75261\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.9335 - acc: 0.7133 - val_loss: 0.8899 - val_acc: 0.7788\n",
            "Epoch 170/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 0.8829 - acc: 0.7273\n",
            "Epoch 170: val_loss did not improve from 0.75261\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.8843 - acc: 0.7274 - val_loss: 0.8231 - val_acc: 0.8173\n",
            "Epoch 171/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 0.9181 - acc: 0.7201\n",
            "Epoch 171: val_loss did not improve from 0.75261\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.9181 - acc: 0.7201 - val_loss: 0.9040 - val_acc: 0.7596\n",
            "Epoch 172/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 0.9090 - acc: 0.7210\n",
            "Epoch 172: val_loss did not improve from 0.75261\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.9090 - acc: 0.7210 - val_loss: 0.8084 - val_acc: 0.7596\n",
            "Epoch 173/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 0.8738 - acc: 0.7388\n",
            "Epoch 173: val_loss did not improve from 0.75261\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.8738 - acc: 0.7388 - val_loss: 0.7770 - val_acc: 0.7885\n",
            "Epoch 174/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 0.8882 - acc: 0.7226\n",
            "Epoch 174: val_loss did not improve from 0.75261\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.8872 - acc: 0.7231 - val_loss: 0.8798 - val_acc: 0.7548\n",
            "Epoch 175/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 0.9171 - acc: 0.7154\n",
            "Epoch 175: val_loss did not improve from 0.75261\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.9171 - acc: 0.7154 - val_loss: 0.8225 - val_acc: 0.7500\n",
            "Epoch 176/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 0.8907 - acc: 0.7256\n",
            "Epoch 176: val_loss did not improve from 0.75261\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.8907 - acc: 0.7256 - val_loss: 0.7935 - val_acc: 0.7837\n",
            "Epoch 177/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 0.8498 - acc: 0.7329\n",
            "Epoch 177: val_loss did not improve from 0.75261\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.8498 - acc: 0.7329 - val_loss: 0.7998 - val_acc: 0.7933\n",
            "Epoch 178/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 0.8600 - acc: 0.7260\n",
            "Epoch 178: val_loss did not improve from 0.75261\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.8629 - acc: 0.7256 - val_loss: 0.8575 - val_acc: 0.7740\n",
            "Epoch 179/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 0.8763 - acc: 0.7384\n",
            "Epoch 179: val_loss did not improve from 0.75261\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.8763 - acc: 0.7384 - val_loss: 0.9072 - val_acc: 0.7692\n",
            "Epoch 180/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 0.8421 - acc: 0.7268\n",
            "Epoch 180: val_loss did not improve from 0.75261\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.8399 - acc: 0.7278 - val_loss: 0.7699 - val_acc: 0.7644\n",
            "Epoch 181/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 0.8585 - acc: 0.7342\n",
            "Epoch 181: val_loss did not improve from 0.75261\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.8585 - acc: 0.7342 - val_loss: 0.9250 - val_acc: 0.7837\n",
            "Epoch 182/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 0.8600 - acc: 0.7286\n",
            "Epoch 182: val_loss improved from 0.75261 to 0.72771, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.8600 - acc: 0.7286 - val_loss: 0.7277 - val_acc: 0.7885\n",
            "Epoch 183/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 0.8186 - acc: 0.7490\n",
            "Epoch 183: val_loss did not improve from 0.72771\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.8186 - acc: 0.7490 - val_loss: 0.7694 - val_acc: 0.7933\n",
            "Epoch 184/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 0.8149 - acc: 0.7422\n",
            "Epoch 184: val_loss improved from 0.72771 to 0.69453, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.8137 - acc: 0.7427 - val_loss: 0.6945 - val_acc: 0.8317\n",
            "Epoch 185/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 0.8184 - acc: 0.7550\n",
            "Epoch 185: val_loss did not improve from 0.69453\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.8184 - acc: 0.7550 - val_loss: 0.7921 - val_acc: 0.8269\n",
            "Epoch 186/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 0.8345 - acc: 0.7401\n",
            "Epoch 186: val_loss did not improve from 0.69453\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.8345 - acc: 0.7401 - val_loss: 0.7634 - val_acc: 0.8125\n",
            "Epoch 187/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 0.8040 - acc: 0.7465\n",
            "Epoch 187: val_loss did not improve from 0.69453\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.8021 - acc: 0.7473 - val_loss: 0.8057 - val_acc: 0.7885\n",
            "Epoch 188/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 0.8042 - acc: 0.7405\n",
            "Epoch 188: val_loss did not improve from 0.69453\n",
            "588/588 [==============================] - 14s 25ms/step - loss: 0.8042 - acc: 0.7405 - val_loss: 0.7931 - val_acc: 0.8077\n",
            "Epoch 189/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 0.8385 - acc: 0.7461\n",
            "Epoch 189: val_loss did not improve from 0.69453\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.8385 - acc: 0.7461 - val_loss: 0.6971 - val_acc: 0.8317\n",
            "Epoch 190/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 0.7999 - acc: 0.7426\n",
            "Epoch 190: val_loss did not improve from 0.69453\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.8001 - acc: 0.7427 - val_loss: 0.7006 - val_acc: 0.8413\n",
            "Epoch 191/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 0.7554 - acc: 0.7546\n",
            "Epoch 191: val_loss did not improve from 0.69453\n",
            "588/588 [==============================] - 14s 25ms/step - loss: 0.7554 - acc: 0.7546 - val_loss: 0.8895 - val_acc: 0.7981\n",
            "Epoch 192/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 0.8040 - acc: 0.7542\n",
            "Epoch 192: val_loss did not improve from 0.69453\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.8068 - acc: 0.7533 - val_loss: 0.9435 - val_acc: 0.7644\n",
            "Epoch 193/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 0.7697 - acc: 0.7665\n",
            "Epoch 193: val_loss did not improve from 0.69453\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.7697 - acc: 0.7665 - val_loss: 0.8161 - val_acc: 0.7885\n",
            "Epoch 194/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 0.7936 - acc: 0.7563\n",
            "Epoch 194: val_loss did not improve from 0.69453\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.7936 - acc: 0.7563 - val_loss: 0.7616 - val_acc: 0.8029\n",
            "Epoch 195/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 0.7356 - acc: 0.7678\n",
            "Epoch 195: val_loss did not improve from 0.69453\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.7356 - acc: 0.7678 - val_loss: 0.8021 - val_acc: 0.7692\n",
            "Epoch 196/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 0.7569 - acc: 0.7656\n",
            "Epoch 196: val_loss did not improve from 0.69453\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.7569 - acc: 0.7656 - val_loss: 0.8432 - val_acc: 0.8125\n",
            "Epoch 197/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 0.7700 - acc: 0.7593\n",
            "Epoch 197: val_loss did not improve from 0.69453\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.7700 - acc: 0.7593 - val_loss: 0.7349 - val_acc: 0.7933\n",
            "Epoch 198/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 0.7689 - acc: 0.7610\n",
            "Epoch 198: val_loss did not improve from 0.69453\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.7694 - acc: 0.7605 - val_loss: 0.8519 - val_acc: 0.7692\n",
            "Epoch 199/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 0.7690 - acc: 0.7614\n",
            "Epoch 199: val_loss did not improve from 0.69453\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.7688 - acc: 0.7614 - val_loss: 0.9049 - val_acc: 0.7788\n",
            "Epoch 200/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 0.7456 - acc: 0.7635\n",
            "Epoch 200: val_loss did not improve from 0.69453\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.7456 - acc: 0.7635 - val_loss: 0.8794 - val_acc: 0.8077\n",
            "Epoch 201/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 0.6997 - acc: 0.7844\n",
            "Epoch 201: val_loss did not improve from 0.69453\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.7009 - acc: 0.7839 - val_loss: 0.7530 - val_acc: 0.8077\n",
            "Epoch 202/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 0.8041 - acc: 0.7499\n",
            "Epoch 202: val_loss did not improve from 0.69453\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.8043 - acc: 0.7499 - val_loss: 0.7293 - val_acc: 0.8125\n",
            "Epoch 203/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 0.7064 - acc: 0.7742\n",
            "Epoch 203: val_loss did not improve from 0.69453\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.7068 - acc: 0.7737 - val_loss: 0.7209 - val_acc: 0.8029\n",
            "Epoch 204/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 0.7069 - acc: 0.7708\n",
            "Epoch 204: val_loss did not improve from 0.69453\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.7101 - acc: 0.7712 - val_loss: 0.8010 - val_acc: 0.8173\n",
            "Epoch 205/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 0.7476 - acc: 0.7687\n",
            "Epoch 205: val_loss did not improve from 0.69453\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.7494 - acc: 0.7678 - val_loss: 0.7942 - val_acc: 0.7981\n",
            "Epoch 206/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 0.7417 - acc: 0.7712\n",
            "Epoch 206: val_loss did not improve from 0.69453\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.7402 - acc: 0.7720 - val_loss: 0.8123 - val_acc: 0.7981\n",
            "Epoch 207/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 0.7153 - acc: 0.7733\n",
            "Epoch 207: val_loss improved from 0.69453 to 0.69267, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.7153 - acc: 0.7733 - val_loss: 0.6927 - val_acc: 0.8221\n",
            "Epoch 208/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 0.7022 - acc: 0.7827\n",
            "Epoch 208: val_loss improved from 0.69267 to 0.67608, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.7027 - acc: 0.7826 - val_loss: 0.6761 - val_acc: 0.8173\n",
            "Epoch 209/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 0.7080 - acc: 0.7785\n",
            "Epoch 209: val_loss did not improve from 0.67608\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.7088 - acc: 0.7780 - val_loss: 0.7767 - val_acc: 0.8029\n",
            "Epoch 210/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 0.7048 - acc: 0.7776\n",
            "Epoch 210: val_loss did not improve from 0.67608\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.7094 - acc: 0.7767 - val_loss: 0.7534 - val_acc: 0.8221\n",
            "Epoch 211/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 0.7388 - acc: 0.7768\n",
            "Epoch 211: val_loss improved from 0.67608 to 0.66534, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.7397 - acc: 0.7767 - val_loss: 0.6653 - val_acc: 0.8221\n",
            "Epoch 212/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 0.7248 - acc: 0.7755\n",
            "Epoch 212: val_loss improved from 0.66534 to 0.61311, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.7246 - acc: 0.7754 - val_loss: 0.6131 - val_acc: 0.8269\n",
            "Epoch 213/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 0.7038 - acc: 0.7784\n",
            "Epoch 213: val_loss did not improve from 0.61311\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.7047 - acc: 0.7780 - val_loss: 0.6688 - val_acc: 0.8510\n",
            "Epoch 214/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 0.6790 - acc: 0.7815\n",
            "Epoch 214: val_loss did not improve from 0.61311\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.6793 - acc: 0.7809 - val_loss: 0.7462 - val_acc: 0.8269\n",
            "Epoch 215/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 0.6940 - acc: 0.7767\n",
            "Epoch 215: val_loss did not improve from 0.61311\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.6929 - acc: 0.7771 - val_loss: 0.6912 - val_acc: 0.8365\n",
            "Epoch 216/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 0.6485 - acc: 0.7942\n",
            "Epoch 216: val_loss did not improve from 0.61311\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.6476 - acc: 0.7946 - val_loss: 0.6800 - val_acc: 0.8221\n",
            "Epoch 217/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 0.6562 - acc: 0.7909\n",
            "Epoch 217: val_loss did not improve from 0.61311\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.6567 - acc: 0.7912 - val_loss: 0.7076 - val_acc: 0.8077\n",
            "Epoch 218/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 0.6498 - acc: 0.7973\n",
            "Epoch 218: val_loss did not improve from 0.61311\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.6540 - acc: 0.7967 - val_loss: 0.7354 - val_acc: 0.8221\n",
            "Epoch 219/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 0.6648 - acc: 0.7877\n",
            "Epoch 219: val_loss did not improve from 0.61311\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.6648 - acc: 0.7877 - val_loss: 0.7011 - val_acc: 0.8173\n",
            "Epoch 220/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 0.6622 - acc: 0.7810\n",
            "Epoch 220: val_loss did not improve from 0.61311\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.6615 - acc: 0.7814 - val_loss: 0.8277 - val_acc: 0.8173\n",
            "Epoch 221/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 0.6662 - acc: 0.8005\n",
            "Epoch 221: val_loss did not improve from 0.61311\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.6662 - acc: 0.8005 - val_loss: 0.7781 - val_acc: 0.8221\n",
            "Epoch 222/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 0.6473 - acc: 0.7916\n",
            "Epoch 222: val_loss did not improve from 0.61311\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.6473 - acc: 0.7916 - val_loss: 0.6858 - val_acc: 0.8317\n",
            "Epoch 223/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 0.6632 - acc: 0.8026\n",
            "Epoch 223: val_loss did not improve from 0.61311\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.6632 - acc: 0.8026 - val_loss: 0.7169 - val_acc: 0.8029\n",
            "Epoch 224/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 0.6571 - acc: 0.7997\n",
            "Epoch 224: val_loss did not improve from 0.61311\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.6562 - acc: 0.8001 - val_loss: 0.7035 - val_acc: 0.8029\n",
            "Epoch 225/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 0.6747 - acc: 0.7895\n",
            "Epoch 225: val_loss did not improve from 0.61311\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.6741 - acc: 0.7899 - val_loss: 0.7612 - val_acc: 0.8173\n",
            "Epoch 226/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 0.6184 - acc: 0.7997\n",
            "Epoch 226: val_loss did not improve from 0.61311\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.6179 - acc: 0.8001 - val_loss: 0.7208 - val_acc: 0.8077\n",
            "Epoch 227/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 0.5950 - acc: 0.8095\n",
            "Epoch 227: val_loss did not improve from 0.61311\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.5948 - acc: 0.8094 - val_loss: 0.8239 - val_acc: 0.8221\n",
            "Epoch 228/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 0.6425 - acc: 0.7943\n",
            "Epoch 228: val_loss did not improve from 0.61311\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.6437 - acc: 0.7941 - val_loss: 0.6697 - val_acc: 0.8269\n",
            "Epoch 229/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 0.6262 - acc: 0.8035\n",
            "Epoch 229: val_loss did not improve from 0.61311\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.6262 - acc: 0.8035 - val_loss: 0.6969 - val_acc: 0.8125\n",
            "Epoch 230/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 0.5954 - acc: 0.8193\n",
            "Epoch 230: val_loss did not improve from 0.61311\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.5951 - acc: 0.8192 - val_loss: 0.6302 - val_acc: 0.8125\n",
            "Epoch 231/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 0.6197 - acc: 0.8036\n",
            "Epoch 231: val_loss improved from 0.61311 to 0.60877, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.6192 - acc: 0.8039 - val_loss: 0.6088 - val_acc: 0.8317\n",
            "Epoch 232/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 0.6517 - acc: 0.7909\n",
            "Epoch 232: val_loss did not improve from 0.60877\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.6507 - acc: 0.7912 - val_loss: 0.6358 - val_acc: 0.8029\n",
            "Epoch 233/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 0.6204 - acc: 0.8014\n",
            "Epoch 233: val_loss did not improve from 0.60877\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.6204 - acc: 0.8014 - val_loss: 0.8321 - val_acc: 0.7933\n",
            "Epoch 234/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 0.6209 - acc: 0.7980\n",
            "Epoch 234: val_loss did not improve from 0.60877\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.6207 - acc: 0.7984 - val_loss: 0.8493 - val_acc: 0.8173\n",
            "Epoch 235/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 0.5707 - acc: 0.8271\n",
            "Epoch 235: val_loss did not improve from 0.60877\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.5769 - acc: 0.8260 - val_loss: 0.8222 - val_acc: 0.7933\n",
            "Epoch 236/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 0.5882 - acc: 0.8078\n",
            "Epoch 236: val_loss did not improve from 0.60877\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.5876 - acc: 0.8082 - val_loss: 0.7384 - val_acc: 0.8413\n",
            "Epoch 237/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 0.5895 - acc: 0.8210\n",
            "Epoch 237: val_loss did not improve from 0.60877\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.5889 - acc: 0.8209 - val_loss: 0.6976 - val_acc: 0.8365\n",
            "Epoch 238/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 0.5786 - acc: 0.8202\n",
            "Epoch 238: val_loss did not improve from 0.60877\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.5777 - acc: 0.8205 - val_loss: 0.7904 - val_acc: 0.8221\n",
            "Epoch 239/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 0.6179 - acc: 0.8185\n",
            "Epoch 239: val_loss did not improve from 0.60877\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.6180 - acc: 0.8184 - val_loss: 0.6721 - val_acc: 0.8221\n",
            "Epoch 240/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 0.5907 - acc: 0.8160\n",
            "Epoch 240: val_loss did not improve from 0.60877\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.5896 - acc: 0.8167 - val_loss: 0.7498 - val_acc: 0.8317\n",
            "Epoch 241/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 0.5757 - acc: 0.8257\n",
            "Epoch 241: val_loss did not improve from 0.60877\n",
            "588/588 [==============================] - 14s 25ms/step - loss: 0.5760 - acc: 0.8252 - val_loss: 0.6657 - val_acc: 0.8413\n",
            "Epoch 242/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 0.5480 - acc: 0.8280\n",
            "Epoch 242: val_loss improved from 0.60877 to 0.58609, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.5504 - acc: 0.8273 - val_loss: 0.5861 - val_acc: 0.8462\n",
            "Epoch 243/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 0.5882 - acc: 0.8168\n",
            "Epoch 243: val_loss did not improve from 0.58609\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.5900 - acc: 0.8158 - val_loss: 0.6747 - val_acc: 0.8077\n",
            "Epoch 244/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 0.5637 - acc: 0.8212\n",
            "Epoch 244: val_loss did not improve from 0.58609\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.5622 - acc: 0.8218 - val_loss: 0.7074 - val_acc: 0.8317\n",
            "Epoch 245/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 0.5363 - acc: 0.8308\n",
            "Epoch 245: val_loss did not improve from 0.58609\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.5356 - acc: 0.8311 - val_loss: 0.6425 - val_acc: 0.8654\n",
            "Epoch 246/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 0.5765 - acc: 0.8176\n",
            "Epoch 246: val_loss did not improve from 0.58609\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.5783 - acc: 0.8171 - val_loss: 0.6964 - val_acc: 0.8077\n",
            "Epoch 247/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 0.5941 - acc: 0.8111\n",
            "Epoch 247: val_loss did not improve from 0.58609\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.5941 - acc: 0.8111 - val_loss: 0.7211 - val_acc: 0.8221\n",
            "Epoch 248/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 0.5622 - acc: 0.8277\n",
            "Epoch 248: val_loss did not improve from 0.58609\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.5622 - acc: 0.8277 - val_loss: 0.7205 - val_acc: 0.8077\n",
            "Epoch 249/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 0.5848 - acc: 0.8147\n",
            "Epoch 249: val_loss did not improve from 0.58609\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.5850 - acc: 0.8145 - val_loss: 0.6185 - val_acc: 0.8221\n",
            "Epoch 250/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 0.5904 - acc: 0.8158\n",
            "Epoch 250: val_loss did not improve from 0.58609\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.5904 - acc: 0.8158 - val_loss: 0.7104 - val_acc: 0.8317\n",
            "Epoch 251/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 0.5527 - acc: 0.8293\n",
            "Epoch 251: val_loss did not improve from 0.58609\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.5515 - acc: 0.8299 - val_loss: 0.6235 - val_acc: 0.8606\n",
            "Epoch 252/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 0.5591 - acc: 0.8273\n",
            "Epoch 252: val_loss improved from 0.58609 to 0.54572, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.5591 - acc: 0.8273 - val_loss: 0.5457 - val_acc: 0.8365\n",
            "Epoch 253/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 0.5082 - acc: 0.8399\n",
            "Epoch 253: val_loss did not improve from 0.54572\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.5077 - acc: 0.8401 - val_loss: 0.7253 - val_acc: 0.8269\n",
            "Epoch 254/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 0.5327 - acc: 0.8235\n",
            "Epoch 254: val_loss did not improve from 0.54572\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.5327 - acc: 0.8235 - val_loss: 0.7620 - val_acc: 0.8173\n",
            "Epoch 255/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 0.5781 - acc: 0.8259\n",
            "Epoch 255: val_loss did not improve from 0.54572\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.5766 - acc: 0.8265 - val_loss: 0.7049 - val_acc: 0.8029\n",
            "Epoch 256/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 0.5530 - acc: 0.8253\n",
            "Epoch 256: val_loss did not improve from 0.54572\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.5522 - acc: 0.8256 - val_loss: 0.6267 - val_acc: 0.8558\n",
            "Epoch 257/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 0.4915 - acc: 0.8464\n",
            "Epoch 257: val_loss did not improve from 0.54572\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.4909 - acc: 0.8464 - val_loss: 0.6016 - val_acc: 0.8365\n",
            "Epoch 258/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 0.5379 - acc: 0.8294\n",
            "Epoch 258: val_loss did not improve from 0.54572\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.5379 - acc: 0.8294 - val_loss: 0.8076 - val_acc: 0.8317\n",
            "Epoch 259/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 0.5508 - acc: 0.8242\n",
            "Epoch 259: val_loss did not improve from 0.54572\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.5491 - acc: 0.8248 - val_loss: 0.5643 - val_acc: 0.8365\n",
            "Epoch 260/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 0.5301 - acc: 0.8276\n",
            "Epoch 260: val_loss did not improve from 0.54572\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.5308 - acc: 0.8273 - val_loss: 0.7900 - val_acc: 0.7885\n",
            "Epoch 261/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 0.5904 - acc: 0.8137\n",
            "Epoch 261: val_loss did not improve from 0.54572\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.5904 - acc: 0.8137 - val_loss: 0.6940 - val_acc: 0.7981\n",
            "Epoch 262/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 0.5184 - acc: 0.8343\n",
            "Epoch 262: val_loss did not improve from 0.54572\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.5189 - acc: 0.8341 - val_loss: 0.7023 - val_acc: 0.8125\n",
            "Epoch 263/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 0.5384 - acc: 0.8335\n",
            "Epoch 263: val_loss did not improve from 0.54572\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.5380 - acc: 0.8337 - val_loss: 0.5858 - val_acc: 0.8365\n",
            "Epoch 264/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 0.5237 - acc: 0.8321\n",
            "Epoch 264: val_loss did not improve from 0.54572\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.5235 - acc: 0.8320 - val_loss: 0.6722 - val_acc: 0.8173\n",
            "Epoch 265/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 0.5015 - acc: 0.8445\n",
            "Epoch 265: val_loss did not improve from 0.54572\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.5010 - acc: 0.8443 - val_loss: 0.6959 - val_acc: 0.8221\n",
            "Epoch 266/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 0.5209 - acc: 0.8337\n",
            "Epoch 266: val_loss did not improve from 0.54572\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.5209 - acc: 0.8337 - val_loss: 0.8230 - val_acc: 0.8173\n",
            "Epoch 267/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 0.5138 - acc: 0.8429\n",
            "Epoch 267: val_loss did not improve from 0.54572\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.5124 - acc: 0.8435 - val_loss: 0.6307 - val_acc: 0.8510\n",
            "Epoch 268/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 0.4845 - acc: 0.8445\n",
            "Epoch 268: val_loss did not improve from 0.54572\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.4855 - acc: 0.8443 - val_loss: 0.7484 - val_acc: 0.8077\n",
            "Epoch 269/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 0.4855 - acc: 0.8469\n",
            "Epoch 269: val_loss did not improve from 0.54572\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.4855 - acc: 0.8469 - val_loss: 0.6630 - val_acc: 0.8317\n",
            "Epoch 270/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 0.5169 - acc: 0.8355\n",
            "Epoch 270: val_loss did not improve from 0.54572\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.5174 - acc: 0.8354 - val_loss: 0.6676 - val_acc: 0.8462\n",
            "Epoch 271/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 0.5244 - acc: 0.8381\n",
            "Epoch 271: val_loss did not improve from 0.54572\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.5248 - acc: 0.8379 - val_loss: 0.6863 - val_acc: 0.8173\n",
            "Epoch 272/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 0.5241 - acc: 0.8412\n",
            "Epoch 272: val_loss did not improve from 0.54572\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.5239 - acc: 0.8409 - val_loss: 0.6695 - val_acc: 0.8317\n",
            "Epoch 273/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 0.5037 - acc: 0.8382\n",
            "Epoch 273: val_loss did not improve from 0.54572\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.5051 - acc: 0.8375 - val_loss: 0.6764 - val_acc: 0.8365\n",
            "Epoch 274/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 0.4956 - acc: 0.8389\n",
            "Epoch 274: val_loss did not improve from 0.54572\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.4949 - acc: 0.8392 - val_loss: 0.6940 - val_acc: 0.8413\n",
            "Epoch 275/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 0.4647 - acc: 0.8573\n",
            "Epoch 275: val_loss improved from 0.54572 to 0.51226, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.4645 - acc: 0.8575 - val_loss: 0.5123 - val_acc: 0.8702\n",
            "Epoch 276/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 0.4698 - acc: 0.8515\n",
            "Epoch 276: val_loss did not improve from 0.51226\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.4686 - acc: 0.8516 - val_loss: 0.5297 - val_acc: 0.8558\n",
            "Epoch 277/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 0.5001 - acc: 0.8422\n",
            "Epoch 277: val_loss did not improve from 0.51226\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.5001 - acc: 0.8422 - val_loss: 0.5677 - val_acc: 0.8702\n",
            "Epoch 278/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 0.4885 - acc: 0.8379\n",
            "Epoch 278: val_loss did not improve from 0.51226\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.4885 - acc: 0.8379 - val_loss: 0.5598 - val_acc: 0.8702\n",
            "Epoch 279/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 0.4821 - acc: 0.8439\n",
            "Epoch 279: val_loss did not improve from 0.51226\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.4821 - acc: 0.8439 - val_loss: 0.6392 - val_acc: 0.8317\n",
            "Epoch 280/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 0.4846 - acc: 0.8470\n",
            "Epoch 280: val_loss did not improve from 0.51226\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.4844 - acc: 0.8473 - val_loss: 0.5718 - val_acc: 0.8654\n",
            "Epoch 281/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 0.4967 - acc: 0.8399\n",
            "Epoch 281: val_loss did not improve from 0.51226\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.4962 - acc: 0.8396 - val_loss: 0.6857 - val_acc: 0.8558\n",
            "Epoch 282/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 0.4868 - acc: 0.8458\n",
            "Epoch 282: val_loss improved from 0.51226 to 0.47080, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.4899 - acc: 0.8456 - val_loss: 0.4708 - val_acc: 0.8702\n",
            "Epoch 283/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 0.4560 - acc: 0.8537\n",
            "Epoch 283: val_loss did not improve from 0.47080\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.4560 - acc: 0.8537 - val_loss: 0.6239 - val_acc: 0.8462\n",
            "Epoch 284/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 0.5016 - acc: 0.8528\n",
            "Epoch 284: val_loss improved from 0.47080 to 0.46212, saving model to tmp_checkpoint.ckpt\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.5016 - acc: 0.8528 - val_loss: 0.4621 - val_acc: 0.8702\n",
            "Epoch 285/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 0.4706 - acc: 0.8503\n",
            "Epoch 285: val_loss did not improve from 0.46212\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.4706 - acc: 0.8503 - val_loss: 0.6882 - val_acc: 0.8558\n",
            "Epoch 286/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 0.4408 - acc: 0.8524\n",
            "Epoch 286: val_loss did not improve from 0.46212\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.4408 - acc: 0.8524 - val_loss: 0.6235 - val_acc: 0.8269\n",
            "Epoch 287/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 0.4648 - acc: 0.8509\n",
            "Epoch 287: val_loss did not improve from 0.46212\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.4643 - acc: 0.8507 - val_loss: 0.5420 - val_acc: 0.8558\n",
            "Epoch 288/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 0.4473 - acc: 0.8594\n",
            "Epoch 288: val_loss did not improve from 0.46212\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.4484 - acc: 0.8592 - val_loss: 0.5960 - val_acc: 0.8462\n",
            "Epoch 289/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 0.5048 - acc: 0.8475\n",
            "Epoch 289: val_loss did not improve from 0.46212\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.5071 - acc: 0.8469 - val_loss: 0.6699 - val_acc: 0.8365\n",
            "Epoch 290/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 0.4423 - acc: 0.8473\n",
            "Epoch 290: val_loss did not improve from 0.46212\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.4423 - acc: 0.8473 - val_loss: 0.5645 - val_acc: 0.8654\n",
            "Epoch 291/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 0.4578 - acc: 0.8590\n",
            "Epoch 291: val_loss did not improve from 0.46212\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.4583 - acc: 0.8588 - val_loss: 0.5068 - val_acc: 0.8510\n",
            "Epoch 292/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 0.4448 - acc: 0.8671\n",
            "Epoch 292: val_loss did not improve from 0.46212\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.4454 - acc: 0.8669 - val_loss: 0.6215 - val_acc: 0.8702\n",
            "Epoch 293/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 0.4272 - acc: 0.8598\n",
            "Epoch 293: val_loss did not improve from 0.46212\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.4265 - acc: 0.8601 - val_loss: 0.5936 - val_acc: 0.8462\n",
            "Epoch 294/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 0.4598 - acc: 0.8558\n",
            "Epoch 294: val_loss did not improve from 0.46212\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.4598 - acc: 0.8558 - val_loss: 0.7558 - val_acc: 0.8221\n",
            "Epoch 295/300\n",
            "588/588 [==============================] - ETA: 0s - loss: 0.4500 - acc: 0.8609\n",
            "Epoch 295: val_loss did not improve from 0.46212\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.4500 - acc: 0.8609 - val_loss: 0.5496 - val_acc: 0.8558\n",
            "Epoch 296/300\n",
            "586/588 [============================>.] - ETA: 0s - loss: 0.4446 - acc: 0.8596\n",
            "Epoch 296: val_loss did not improve from 0.46212\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.4442 - acc: 0.8596 - val_loss: 0.6324 - val_acc: 0.8654\n",
            "Epoch 297/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 0.4335 - acc: 0.8641\n",
            "Epoch 297: val_loss did not improve from 0.46212\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.4332 - acc: 0.8643 - val_loss: 0.5161 - val_acc: 0.8750\n",
            "Epoch 298/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 0.4411 - acc: 0.8645\n",
            "Epoch 298: val_loss did not improve from 0.46212\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.4407 - acc: 0.8643 - val_loss: 0.6016 - val_acc: 0.8558\n",
            "Epoch 299/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 0.4340 - acc: 0.8620\n",
            "Epoch 299: val_loss did not improve from 0.46212\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.4338 - acc: 0.8622 - val_loss: 0.6981 - val_acc: 0.8510\n",
            "Epoch 300/300\n",
            "587/588 [============================>.] - ETA: 0s - loss: 0.4444 - acc: 0.8564\n",
            "Epoch 300: val_loss did not improve from 0.46212\n",
            "588/588 [==============================] - 15s 25ms/step - loss: 0.4452 - acc: 0.8562 - val_loss: 0.6304 - val_acc: 0.8558\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(training_generator, \n",
        "                    validation_data=(validation_generator),\n",
        "                    epochs=epochs,\n",
        "                    callbacks=[checkpoint],\n",
        "                    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(history.history.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdCPCpyQ8P8m",
        "outputId": "4a0f1007-e2a3-4b79-e20d-4befb8405149"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['loss', 'acc', 'val_loss', 'val_acc'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['loss']) # loss 그래프\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "JwbxRf5M8Vj2",
        "outputId": "4b660c2c-9513-4264-e2a7-9feeeaeaf431"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['acc']) # acc 그래프\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('acc')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "-Rt-jTrJ8WKi",
        "outputId": "0dafa45c-7467-404d-e7e1-1bf0014378b0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['val_loss']) # val_loss 그래프\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('val_loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "2EpQ3sQ68WTU",
        "outputId": "48caff22-8310-4761-f40d-31f2909cbf03"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['val_acc']) # val_acc 그래프\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('val_acc')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "KyJ6Qs7t8Wee",
        "outputId": "1947b6d4-e7e8-4d95-ba2a-a883ca45a0f8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdPDX-4Ckvuk"
      },
      "source": [
        "## Load Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "dKwAHJDxkxdO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e048ecdf-07f5-4898-a7ee-d8d1df99d096"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f52739f7dd0>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "model.load_weights(checkpoint_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "tDXU7MTekx0_"
      },
      "outputs": [],
      "source": [
        "model.save(\"부천고_road_sign_detector.h5\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "StKBkrptVQWW",
        "Wc2sry-YVhmq",
        "Hs0lO6JIUnQk",
        "x52bGw75VM0i",
        "YdPDX-4Ckvuk"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "acadd50720a0652b33f7cbf327bae167d6ca1cf8c0320c5affbfd685656a1fe7"
      }
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}